{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb41e6f0-df0a-4e41-910b-c0a933a649ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6398e50e-fefe-4aeb-8638-e3d8ef0f3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5deeed00-ff4b-46f7-acb2-b0ff90e7108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window = 5\n",
    "def create_context_target_pairs(doc, context_window):\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    pairs = []\n",
    "    for i in range(context_window, len(words)):\n",
    "        context = words[i - context_window:i]\n",
    "        target = words[i]\n",
    "        pairs.append((context, target))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48aeb7c-42de-41f6-bcd8-9c269ec3c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ENTRE COMPUTER CENTERS INC & \t Target: lt\n",
      "Context: COMPUTER CENTERS INC & lt \t Target: ;\n",
      "Context: CENTERS INC & lt ; \t Target: ETRE.O\n",
      "Context: INC & lt ; ETRE.O \t Target: >\n",
      "Context: & lt ; ETRE.O > \t Target: 2ND\n"
     ]
    }
   ],
   "source": [
    "random_doc_id = random.choice(reuters.fileids())\n",
    "document = reuters.raw(random_doc_id)\n",
    "context_target_pairs = create_context_target_pairs(document, context_window)\n",
    "for context, target in context_target_pairs[:5]:\n",
    "    print(f\"Context: {' '.join(context)} \\t Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef120119-cea0-43b8-ac32-a5641564488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Text: ['ENTRE', 'COMPUTER', 'CENTERS', 'INC', '&']\n",
      "Target Text: lt\n",
      "Context Text: ['COMPUTER', 'CENTERS', 'INC', '&', 'lt']\n",
      "Target Text: ;\n",
      "Context Text: ['CENTERS', 'INC', '&', 'lt', ';']\n",
      "Target Text: ETRE.O\n",
      "Context Text: ['INC', '&', 'lt', ';', 'ETRE.O']\n",
      "Target Text: >\n",
      "Context Text: ['&', 'lt', ';', 'ETRE.O', '>']\n",
      "Target Text: 2ND\n",
      "Context Text: ['lt', ';', 'ETRE.O', '>', '2ND']\n",
      "Target Text: QTR\n",
      "Context Text: [';', 'ETRE.O', '>', '2ND', 'QTR']\n",
      "Target Text: LOSS\n",
      "Context Text: ['ETRE.O', '>', '2ND', 'QTR', 'LOSS']\n",
      "Target Text: Ended\n",
      "Context Text: ['>', '2ND', 'QTR', 'LOSS', 'Ended']\n",
      "Target Text: Feb\n",
      "Context Text: ['2ND', 'QTR', 'LOSS', 'Ended', 'Feb']\n",
      "Target Text: 28\n",
      "Context Text: ['QTR', 'LOSS', 'Ended', 'Feb', '28']\n",
      "Target Text: Shr\n",
      "Context Text: ['LOSS', 'Ended', 'Feb', '28', 'Shr']\n",
      "Target Text: loss\n",
      "Context Text: ['Ended', 'Feb', '28', 'Shr', 'loss']\n",
      "Target Text: 29\n",
      "Context Text: ['Feb', '28', 'Shr', 'loss', '29']\n",
      "Target Text: cts\n",
      "Context Text: ['28', 'Shr', 'loss', '29', 'cts']\n",
      "Target Text: vs\n",
      "Context Text: ['Shr', 'loss', '29', 'cts', 'vs']\n",
      "Target Text: profit\n",
      "Context Text: ['loss', '29', 'cts', 'vs', 'profit']\n",
      "Target Text: 10\n",
      "Context Text: ['29', 'cts', 'vs', 'profit', '10']\n",
      "Target Text: cts\n",
      "Context Text: ['cts', 'vs', 'profit', '10', 'cts']\n",
      "Target Text: Net\n",
      "Context Text: ['vs', 'profit', '10', 'cts', 'Net']\n",
      "Target Text: loss\n",
      "Context Text: ['profit', '10', 'cts', 'Net', 'loss']\n",
      "Target Text: 2,733,000\n",
      "Context Text: ['10', 'cts', 'Net', 'loss', '2,733,000']\n",
      "Target Text: vs\n",
      "Context Text: ['cts', 'Net', 'loss', '2,733,000', 'vs']\n",
      "Target Text: profit\n",
      "Context Text: ['Net', 'loss', '2,733,000', 'vs', 'profit']\n",
      "Target Text: 911,000\n",
      "Context Text: ['loss', '2,733,000', 'vs', 'profit', '911,000']\n",
      "Target Text: Revs\n",
      "Context Text: ['2,733,000', 'vs', 'profit', '911,000', 'Revs']\n",
      "Target Text: 21.5\n",
      "Context Text: ['vs', 'profit', '911,000', 'Revs', '21.5']\n",
      "Target Text: mln\n",
      "Context Text: ['profit', '911,000', 'Revs', '21.5', 'mln']\n",
      "Target Text: vs\n",
      "Context Text: ['911,000', 'Revs', '21.5', 'mln', 'vs']\n",
      "Target Text: 18.5\n",
      "Context Text: ['Revs', '21.5', 'mln', 'vs', '18.5']\n",
      "Target Text: mln\n",
      "Context Text: ['21.5', 'mln', 'vs', '18.5', 'mln']\n",
      "Target Text: Six\n",
      "Context Text: ['mln', 'vs', '18.5', 'mln', 'Six']\n",
      "Target Text: mths\n",
      "Context Text: ['vs', '18.5', 'mln', 'Six', 'mths']\n",
      "Target Text: Shr\n",
      "Context Text: ['18.5', 'mln', 'Six', 'mths', 'Shr']\n",
      "Target Text: loss\n",
      "Context Text: ['mln', 'Six', 'mths', 'Shr', 'loss']\n",
      "Target Text: 23\n",
      "Context Text: ['Six', 'mths', 'Shr', 'loss', '23']\n",
      "Target Text: cts\n",
      "Context Text: ['mths', 'Shr', 'loss', '23', 'cts']\n",
      "Target Text: vs\n",
      "Context Text: ['Shr', 'loss', '23', 'cts', 'vs']\n",
      "Target Text: profit\n",
      "Context Text: ['loss', '23', 'cts', 'vs', 'profit']\n",
      "Target Text: 26\n",
      "Context Text: ['23', 'cts', 'vs', 'profit', '26']\n",
      "Target Text: cts\n",
      "Context Text: ['cts', 'vs', 'profit', '26', 'cts']\n",
      "Target Text: Net\n",
      "Context Text: ['vs', 'profit', '26', 'cts', 'Net']\n",
      "Target Text: loss\n",
      "Context Text: ['profit', '26', 'cts', 'Net', 'loss']\n",
      "Target Text: 2,154,000\n",
      "Context Text: ['26', 'cts', 'Net', 'loss', '2,154,000']\n",
      "Target Text: vs\n",
      "Context Text: ['cts', 'Net', 'loss', '2,154,000', 'vs']\n",
      "Target Text: profit\n",
      "Context Text: ['Net', 'loss', '2,154,000', 'vs', 'profit']\n",
      "Target Text: 2,445,000\n",
      "Context Text: ['loss', '2,154,000', 'vs', 'profit', '2,445,000']\n",
      "Target Text: Revs\n",
      "Context Text: ['2,154,000', 'vs', 'profit', '2,445,000', 'Revs']\n",
      "Target Text: 37.8\n",
      "Context Text: ['vs', 'profit', '2,445,000', 'Revs', '37.8']\n",
      "Target Text: mln\n",
      "Context Text: ['profit', '2,445,000', 'Revs', '37.8', 'mln']\n",
      "Target Text: vs\n",
      "Context Text: ['2,445,000', 'Revs', '37.8', 'mln', 'vs']\n",
      "Target Text: 37.7\n",
      "Context Text: ['Revs', '37.8', 'mln', 'vs', '37.7']\n",
      "Target Text: mln\n"
     ]
    }
   ],
   "source": [
    "for context_text, target_text in context_target_pairs:\n",
    "    print(f\"Context Text: {context_text}\")\n",
    "    print(f\"Target Text: {target_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40bb6310-dfb9-4c01-a06c-e2a0c3f2740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "max_seq_length = 32\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "labels_list = []\n",
    "for context_words, target_text in context_target_pairs:\n",
    "    context_text = \" \".join(context_words)\n",
    "    \n",
    "    context_tokens = tokenizer.tokenize(context_text)\n",
    "    target_tokens = tokenizer.tokenize(target_text)\n",
    "\n",
    "    if len(context_tokens) + len(target_tokens) > max_seq_length:\n",
    "        context_tokens = context_tokens[-(max_seq_length - len(target_tokens)):]\n",
    "\n",
    "    context_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "    target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "\n",
    "    input_ids = torch.tensor(context_ids + target_ids, dtype=torch.long)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)  \n",
    "    labels = torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "    input_ids_list.append(input_ids)\n",
    "    attention_mask_list.append(attention_mask)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "input_ids = pad_sequence(input_ids_list, batch_first=True)\n",
    "attention_mask = pad_sequence(attention_mask_list, batch_first=True)\n",
    "labels = pad_sequence(labels_list, batch_first=True)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=18, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17912e55-f3ca-4dcc-b42c-967663b36988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (324) to match target batch_size (108).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1380\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()  \u001b[38;5;66;03m# -100 index = padding token\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1383\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (324) to match target batch_size (108)."
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 1\n",
    "\n",
    "num_training_steps = len(dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "num_training_steps = len(dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "output_model_dir = \"G:/Data Projects/next_word_predictor_with_BERT/saved model\"\n",
    "model.save_pretrained(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8643abd-e2e6-4fb2-99db-9208cfdeba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d292663a-9ef7-40b5-a550-9d9a415d1c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 18])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a18b5df-f6ca-4cb6-a253-1f3df0e6ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 18])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
